{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  I. Seminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home/jre'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/local/Cellar/apache-spark/2.2.0/libexec'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/local/bin/python2.7' # because default is python3 for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python\")\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/lib/py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a SparkConf object to configure the application\n",
    "\n",
    "conf = (SparkConf().setMaster(\"local[8]\")     # cluster url\n",
    "                   .setAppName(\"ML demo\")     # application name\n",
    "                   .set(\"spark.executor.memory\", \"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a SparkContext object to work with Spark\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# to shut down Spark call sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize an SQLContext object to work with Spark SQL\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionModel, LinearRegressionWithSGD\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928638123469\n"
     ]
    }
   ],
   "source": [
    "# initialize data\n",
    "data = [\n",
    "    #LabeledPoint(label,features)\n",
    "    LabeledPoint(0.0,[0.0]),\n",
    "    LabeledPoint(1.0,[1.0]),\n",
    "    LabeledPoint(3.0,[2.0]),\n",
    "    LabeledPoint(2.0,[3.0])\n",
    "]\n",
    "\n",
    "# train a model on data\n",
    "lrm = LinearRegressionWithSGD.train(\n",
    "    sc.parallelize(data),\n",
    "    iterations = 10,\n",
    "    initialWeights = np.array([1.0])\n",
    ")\n",
    "\n",
    "# apply model to a single data point to predict its label\n",
    "print(lrm.predict(np.array([1.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load titanic dataset as DataFrame\n",
    "\n",
    "df = sqlcontext.read.format(\n",
    "    'com.databricks.spark.csv').options(\n",
    "    header='true').load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=u'1', Survived=u'0', Pclass=u'3', Name=u'Braund, Mr. Owen Harris', Sex=u'male', Age=u'22', SibSp=u'1', Parch=u'0', Ticket=u'A/5 21171', Fare=u'7.25', Cabin=None, Embarked=u'S'),\n",
       " Row(PassengerId=u'2', Survived=u'1', Pclass=u'1', Name=u'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex=u'female', Age=u'38', SibSp=u'1', Parch=u'0', Ticket=u'PC 17599', Fare=u'71.2833', Cabin=u'C85', Embarked=u'C'),\n",
       " Row(PassengerId=u'3', Survived=u'1', Pclass=u'3', Name=u'Heikkinen, Miss. Laina', Sex=u'female', Age=u'26', SibSp=u'0', Parch=u'0', Ticket=u'STON/O2. 3101282', Fare=u'7.925', Cabin=None, Embarked=u'S')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features based on existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Embarked=u'Q'), Row(Embarked=u'C'), Row(Embarked=u'S'), Row(Embarked=u'')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types\n",
    "\n",
    "# define a udf\n",
    "\n",
    "def Embarked_transform(x):\n",
    "    if x != None:\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "my_udf = udf(Embarked_transform, types.StringType())    # define udf with return type\n",
    "df = df.withColumn('Embarked', my_udf(df['Embarked']))  # replace column 'Embarked' with new contents using udf\n",
    "\n",
    "df.select('Embarked').distinct().collect()  # show distinct values from column 'Embarked'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace every value in column `Embarked` with a one-hot vector because it is a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# assign indexes to values in 'Embarked'\n",
    "stringIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "\n",
    "# define transformation\n",
    "model = stringIndexer.fit(df)\n",
    "\n",
    "# perform transformation\n",
    "indexed = model.transform(df)\n",
    "\n",
    "# do one-hot encoding of a column\n",
    "encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "\n",
    "# perform transformation\n",
    "df_t = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_age(str_age):\n",
    "    '''Return age as float.'''\n",
    "    try:\n",
    "        return float(str_age)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf(r):\n",
    "    '''Return a LabeledPoint with custom features.'''\n",
    "    return LabeledPoint(int(r.Survived), [  int(r.Pclass),\n",
    "                                            r.Sex == 'male',\n",
    "                                            float(r.Fare),\n",
    "                                            int(r.SibSp),\n",
    "                                            int(r.Parch),\n",
    "                                            parse_age(r.Age),] + list(r.EmbarkedVec.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create new dataset with custom features\n",
    "data = df_t.rdd.map(transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Random Forest model on the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into two parts\n",
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[98] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()  # persist this rdd (prevent from recomputing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "# train random forest model on our 'train' data\n",
    "\n",
    "rfc = RandomForest.trainClassifier(train, \n",
    "                                   numClasses=2, \n",
    "                                   categoricalFeaturesInfo={}, \n",
    "                                   numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(rfc, test):\n",
    "    '''Compute accuracy of model 'rfc' applied to 'test' data.'''\n",
    "    values = test.map(lambda x: x.features)      # get features from elements in 'test' (new rdd)\n",
    "    yhat = rfc.predict(values)                   # get predictions via trained random forest model\n",
    "    y = test.map(lambda x: x.label)              # get labels from elements in 'test'\n",
    "    comp = yhat.zip(y)                           # zip predictions with true labels\n",
    "    errors = comp.map(lambda x: abs(x[0]-x[1]))  # compute errors\n",
    "    \n",
    "    return 1 - errors.sum()/errors.count()       # return average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7969924812030076"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model on test data\n",
    "acc(rfc, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II. Home Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add 5 new features\n",
    "    - 3 of which are computed based on existing ones\n",
    "    - at least one uses UDF\n",
    "2. Train 3 new ML models\n",
    "3. Compute F1-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data that we are going to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Variable|Definition|\n",
    "|:---|:---|\n",
    "|Survived|Survival|\n",
    "|Pclass|Ticket class|\n",
    "|Name|Name|\n",
    "|Sex|Sex|\n",
    "|Age|Age in years|\n",
    "|SibSp|# of siblings / spouses aboard the Titanic|\n",
    "|Parch|# of parents / children aboard the Titanic|\n",
    "|Ticket|\tTicket number\t|\n",
    "|Fare|\tPassenger fare\t|\n",
    "|Cabin|\tCabin number\t|\n",
    "|Embarked|\tPort of Embarkation|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make new features in addintion to those that we've defined during class. We'll write a function that extracts existing features and composes a new element with them.\n",
    "\n",
    "Some features are inspired by [an article](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/) on feature engineering for Titanic dataset.\n",
    "\n",
    "1. **Age groups**\n",
    "    - I started with defining children, but then decided to go further and defined a whole bunch of age groups.\n",
    "    - It is a categorical feature and thus needs to be encoded for ML.\n",
    "2. **Women's marriage status**\n",
    "    - I tried to differentiate between married and not married women judging by their titles in names. I realize that it's a bit lazy approach since there are a lot more titles in the dataset but hope it's okay.\n",
    "    - This produces two features that are both numerical.\n",
    "3. **Deck**\n",
    "    - Deck letter roughly corresponds to a floor of the ship and can be obtained directly from the cabin number.\n",
    "4. **Family on board**\n",
    "    - This boolean feature indicates whether a passenger has any family members on board at all, or is considered alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age groups** feature derived using UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def age_group(age_str):\n",
    "    '''Assign an age group based on age.'''\n",
    "    age = parse_age(age_str)\n",
    "    \n",
    "    if age > 0:\n",
    "        if age < 3:\n",
    "            return 'toddler'\n",
    "        elif age < 14:\n",
    "            return 'child'\n",
    "        elif age < 20:\n",
    "            return 'teenager'\n",
    "        elif age < 60:\n",
    "            return 'adult'\n",
    "        else:\n",
    "            return 'oldster'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "age_udf = udf(age_group, types.StringType())              # define udf with return type\n",
    "df_t = df_t.withColumn('AgeGroup', age_udf(df_t['Age']))  # add column 'AgeGroup' using udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace every value in column `AgeGroup` with a one-hot vector because it is a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign indexes to values in 'AgeGroup'\n",
    "stringIndexer = StringIndexer(inputCol=\"AgeGroup\", outputCol=\"AgeGroupIndex\")\n",
    "\n",
    "# define transformation\n",
    "model = stringIndexer.fit(df_t)\n",
    "\n",
    "# perform transformation\n",
    "indexed = model.transform(df_t)\n",
    "\n",
    "# do one-hot encoding of a column\n",
    "encoder = OneHotEncoder(inputCol=\"AgeGroupIndex\", outputCol=\"AgeGroupVec\")\n",
    "\n",
    "# perform transformation\n",
    "df_t = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector, AgeGroup: string, AgeGroupIndex: double, AgeGroupVec: vector]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deck** feature derived using UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deck(cabin_str):\n",
    "    '''Get deck letter based on cabin number.'''\n",
    "    \n",
    "    if cabin_str is not None:\n",
    "        return cabin_str[0]\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "deck_udf = udf(get_deck, types.StringType())             # define udf with return type\n",
    "df_t = df_t.withColumn('Deck', deck_udf(df_t['Cabin']))  # add column 'Deck' using udf    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace every value in column `Deck` with a one-hot vector because it is a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign indexes to values in 'Deck'\n",
    "stringIndexer = StringIndexer(inputCol=\"Deck\", outputCol=\"DeckIndex\")\n",
    "\n",
    "# define transformation\n",
    "model = stringIndexer.fit(df_t)\n",
    "\n",
    "# perform transformation\n",
    "indexed = model.transform(df_t)\n",
    "\n",
    "# do one-hot encoding of a column\n",
    "encoder = OneHotEncoder(inputCol=\"DeckIndex\", outputCol=\"DeckVec\")\n",
    "\n",
    "# perform transformation\n",
    "df_t = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, EmbarkedIndex: double, EmbarkedVec: vector, AgeGroup: string, AgeGroupIndex: double, AgeGroupVec: vector, Deck: string, DeckIndex: double, DeckVec: vector]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final feature definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(r):\n",
    "    '''Return a LabeledPoint with custom features.'''\n",
    "    \n",
    "    name = r.Name\n",
    "    \n",
    "    return LabeledPoint(\n",
    "                        # label and features defined previously\n",
    "                        int(r.Survived), [int(r.Pclass),     # ticket class\n",
    "                                          r.Sex == 'male',   # sex\n",
    "                                          parse_age(r.Age),  # age\n",
    "                                          int(r.SibSp),      # number of siblings\n",
    "                                          int(r.Parch),      # number of parents/children\n",
    "                                          float(r.Fare),     # ticket cost\n",
    "                                          ]\n",
    "                                       + list(r.EmbarkedVec.toArray())  # embarkation location (one-hot vector)\n",
    "                        # new features\n",
    "                                       + ['Miss.' in name or 'Mlle.' in name or 'Ms.' in name,  # not married woman\n",
    "                                          'Mrs.' in name or 'Mme' in name,  # married woman\n",
    "                                          int(r.SibSp)+int(r.Parch) > 0,    # has family members on board\n",
    "                                          ]\n",
    "                                       + list(r.AgeGroupVec.toArray())  # age group (one-hot vector)\n",
    "                                       + list(r.DeckVec.toArray())      # deck (one-hot vector)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create new dataset with custom features\n",
    "data = df_t.rdd.map(extract_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into two parts\n",
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[166] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()  # persist this rdd (prevent from recomputing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(model, test):\n",
    "    '''Compute F1-measure of model applied to test data.'''\n",
    "    # F1 = 2/(1/recall + 1/precision)\n",
    "    # recall = TP/(TP + FN)\n",
    "    # precision = TP/(TP + FP)\n",
    "    \n",
    "    values = test.map(lambda x: x.features)      # get features from elements in 'test' (new rdd)\n",
    "    yhat = model.predict(values)                 # get predictions via trained random forest model\n",
    "    y = test.map(lambda x: x.label)              # get labels from elements in 'test'\n",
    "    comp = yhat.zip(y)                           # zip predictions with true labels\n",
    "    # comp == (prediction, truth)\n",
    "    \n",
    "    tp_set = comp.filter(lambda x: x[0] + x[1] == 2)\n",
    "    tp = float(tp_set.count())\n",
    "    \n",
    "    fp_set = comp.filter(lambda x: x[0] - x[1] == 1)\n",
    "    fp = float(fp_set.count())\n",
    "    \n",
    "    fn_set = comp.filter(lambda x: x[1] - x[0] == 1)\n",
    "    fn = float(fn_set.count())\n",
    "    \n",
    "    if tp > 0:\n",
    "        recall_ = (tp + fn)/tp\n",
    "        precision_ = (tp + fp)/tp\n",
    "        return 2.0/(recall_ + precision_)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "lrm = LogisticRegressionWithLBFGS.train(train, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7439613526570049"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(lrm, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "svm = SVMWithSGD.train(train, iterations=100, miniBatchFraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5617021276595745"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(svm, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "\n",
    "gbt = GradientBoostedTrees.trainClassifier(train, categoricalFeaturesInfo={}, numIterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7922705314009661"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(gbt, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
